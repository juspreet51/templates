{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Biological Neuron: Dendrites -> Neuron Body -> Axons\n",
    "##### Artificial Neuron: Input/s -> Neuron Body -> Output\n",
    "\n",
    "<img src=\"./imgs/basic_neuron.PNG\" style=\"width:500px\">\n",
    "\n",
    "e.g. IP1 = 12 and IP2=4; W1=0.5 & W2=-1 and a bias(b)=1\n",
    "Then Inputs are multiplied by Wieghts\n",
    "\n",
    "So, (12*0.5)+(4*-1) + 1 = (6)+(-4)+1 = 3\n",
    "\n",
    "i.e. $\\sum_{i=0}^n w_{i}x_{i}+b$\n",
    "\n",
    "This will be passed to Activation Function, who will take a decesion in accordance to the value being fed <br>\n",
    "<div class=\"\" style=\"display: inline-block;\">\n",
    "    <img src=\"./imgs/basic_perceptron.png\" style=\"width:300px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Multiple Perceptron Model\n",
    "<div class=\"\" style=\"display: inline-block;\">\n",
    "    <img src=\"./imgs/Multiple_Perceptron_Model.PNG\" style=\"width:300px\">\n",
    "</div>\n",
    "\n",
    "<br>Input layers are Black-Box <br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "**Commonly used Activation Functions:**\n",
    "- **Sigmoid** <br>\n",
    "> It squashes the input in the range [0,1] <br>\n",
    "> Historically popular since they have nice interpretation as a saturating firing rate of a neuron <br>\n",
    "> Saturated neuron kills gradient <br>\n",
    "> slow convergence <br>\n",
    "> Vanishing Gradient Problem <br>\n",
    "<img src=\"./imgs/activation_functions_0.PNG\" style=\"width:300px\">\n",
    "\n",
    "- **Hyperbolic tangent Function tanh(x)** <br>\n",
    "> Squashes input to range [-1, 1]<br>\n",
    "> Zero Centered <br>\n",
    "> Saturated neuron kills gradient <br>\n",
    "<img src=\"./imgs/activation_function_Tanh_Hyperbolic_tangent.png\" style=\"width:300px\">  <br>\n",
    "<br>\n",
    "\n",
    "- **Rectified Linear Unit (ReLU)** <br>\n",
    "> f(x) = max(0, x) <br>\n",
    "> Derivative : 1, if x> 0 else 0  <br>\n",
    "> Range [0, inf ] <br>\n",
    "> Does not saturate ( in + region) <br>\n",
    "> Computationally very efficient <br>\n",
    "> Converges much faster than sigmoid/tanh <br>\n",
    "> Not Zero Centered <br>\n",
    "> Neuron Dies sometimes <br>\n",
    "<br>\n",
    "<img src=\"./imgs/activation_functions_Relu.png\" style=\"width:300px\"> <br>\n",
    "<br>\n",
    "\n",
    "- **Leaky ReLU** <br>\n",
    "> All benefit of ReLU <br>\n",
    "> Will not Die <br>\n",
    "> Parametric ReLU : max(ax,x) , a can be variable <br>\n",
    "<img src= \"./imgs/activation_functions_Leaky_ReLU.jpeg\" style=\"width:300px\"> <br>\n",
    "\n",
    "___\n",
    "## Quick Summary \n",
    "\n",
    "    Use ReLU, but be careful about your learning rate.\n",
    "    Try Out Leaky ReLU/ELU\n",
    "    You can try tanh but don't expect much\n",
    "    Don't use sigmoid in intermediate layers\n",
    "<img src=\"./imgs/activation_functions_1.PNG\" style=\"width:500px\"> <br>\n",
    "\n",
    "\n",
    "___\n",
    "## Traditional V.S. Modern day Activation Functiona\n",
    "<img src=\"./imgs/activation_functions_traditional_vs_modern.png\" style=\"width:500px\">\n",
    "\n",
    "___\n",
    "\n",
    "## Keras has all these Activation Functions built in, so we wold just call them inside our network, as per our requirements\n",
    "## Next: Keras\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

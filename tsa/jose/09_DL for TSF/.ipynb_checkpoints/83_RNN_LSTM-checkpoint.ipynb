{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><b>RNN</b></font>\n",
    "### <font color=\"green\"><b>Steps performed:</b></font>\n",
    "<div style=\"color:cyan;font-weight:bold;\">\n",
    "1) Load dataset<br>\n",
    "2) visualize the feature column<br>\n",
    "3) Plot seasonal decompose<br>\n",
    "4) Split data into Train-Test<br>\n",
    "5) Scale the data<br>\n",
    "6) Feeding batches of data to RNN<br>\n",
    "7) Building LSTM Model <br>\n",
    "8) Visualizing the fitted model<br>\n",
    "9) Evaluation batch on test data<br>\n",
    "10) Forecast using RNN Model<br>\n",
    "11) Inverse Transformations and Comparison<br>\n",
    "12) Saving and loading the model<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Alcohol_Sales.csv\", index_col=\"DATE\", parse_dates=True)\n",
    "df.index.freq=\"MS\"\n",
    "df.columns = ['Sales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(18,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decompositions = seasonal_decompose(df['Sales'])\n",
    "decompositions.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompositions.resid.plot(figsize=(18,9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step4-5: Train-Test Split and Scale Data</b></ins></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.iloc[:313], df.iloc[313:]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Fit only to train dataset, as Test Dataset's scale is unknown\n",
    "scaler.fit(train)\n",
    "\n",
    "scaled_train_data = scaler.transform(train)\n",
    "scaled_test_data = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train-data size: {len(scaled_train_data)}, Test-data size: {len(scaled_test_data)}\")\n",
    "print(f\"Sum: {len(scaled_train_data) + len(scaled_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data_df = pd.DataFrame(scaled_train_data)\n",
    "scaled_train_data_df.columns = train.columns\n",
    "scaled_train_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><b>Step6: Feeding batches of data to RNN via <ins>Time Series Generator</ins></b></font>\n",
    "\n",
    "This class takes in a sequence of data-points gathered at\n",
    "equal intervals, along with time series parameters such as\n",
    "stride, length of history, etc., to produce batches for\n",
    "training/validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For feeding batches of data to RNN\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# usually equal to the seasonal period cycle, i.e. 7 for day level data, 12 for month level data, etc\n",
    "n_timesteps = 12\n",
    "n_features = 1\n",
    "\n",
    "# since source of data and traget are same, pass scaled train dataset twice\n",
    "train_generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_timesteps, batch_size=n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Len of train_dataset = len of generator + len of n_timesteps\")\n",
    "print(f\" {len(scaled_train_data)} = {len(train_generator)} + {n_timesteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What _TimeseriesGenerator_ does for us is tranform the sequence <br>\n",
    "> [t1,t2,t3,t4,t5,t6] into <br>\n",
    "> [t1,t2,t3,t4,t5] -> [t6] <br>\n",
    "i.e. it takes a sequence of data, transforms it to the above format, i.e. the input format of RNN <br>\n",
    "and LHS is features, RHS is label for the RNN for each batch <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_generator[0]\n",
    "print(\"#Length of training data:\\n\",X.flatten()); print(\"\\n#Prdicting:\",y.flatten());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step7: Building the LSTM Model</ins></b></font>\n",
    "__Building a model__ <br>\n",
    "Let's first see what we need to do when we want to train a model\n",
    "\n",
    "- First, we want to decide a model architecture, this is the number of hidden layers and activation functions, etc (___compile___)\n",
    "- Secondly, we will want to train our model to get all the paramters to the correct value to map our inputs to our outputs (___fit___)\n",
    "- Lastly, we will want to use this model to do some feed-forward passes to predict novel inputs (___predict___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"teal\"><b>Input of an LSTM Layer</b></font>\n",
    "\n",
    "The input to every LSTM layer(_input_shape=(s,t,f)_) must be 3D-Array of data:\n",
    "- ___Samples___: One sequence is one sample. A batch is comprised of one or more samples\n",
    "- ___Time-Steps___: One time step is one point of observation in the sample, i.e. at a time, how many point of observations do you need?\n",
    "- ___Features___: One feature is one observation at a time step\n",
    "\n",
    "> For example, the model below defines an input layer that expects 1 or more samples, 50 time steps, and 2 features\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(50, 2)))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "<a id=\"Stacked_LSTM\"></a>\n",
    "### <font color=\"teal\"><b>Stacked LSTM</b></font>\n",
    "\n",
    "Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model\n",
    "> However, LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence\n",
    "\n",
    "`Stacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique\n",
    "It is the depth of neural networks that is generally attributed to the success of the approach on a wide range of challenging prediction problems`\n",
    "\n",
    "> Additional hidden layers can be added to a Multilayer Perceptron neural network to make it deeper<br>\n",
    "\n",
    "> The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction<br>\n",
    "\n",
    "> e.g. from lines to shapes to objects\n",
    "\n",
    "- _We can address this by setting the <font color=\"green\">return_sequences=True</font> argument on the layer and having the LSTM output a value for each time step in the input data_<br>\n",
    "- _This allows us to have 3D output from hidden LSTM layer as input to the next_\n",
    "\n",
    "We can therefore define a Stacked LSTM as follows:\n",
    "```python\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM \n",
    "\n",
    "model = Sequential()\n",
    "# Constructing a Vanila LSTM, i.e. LSTM model that has a single hidden layer of LSTM units,\n",
    "# and an output layer used to make a prediction\n",
    "model.add((LSTM(150, activation='relu', input_shape=(n_timesteps, n_features))))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, epochs=40,verbose=2)\n",
    "epochs_till_now = [40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step8: Visualizing the fitted model</ins></b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step9: Evaluation Batch on Test Data</b></ins></font>\n",
    "Model is working on the monthly data for a sequence of ___(12 hsitory points)___ -> ___(13th point)___ <br>\n",
    "So we have to have last ___(12 point of train data)___ -> to predict ___(13th step)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eval_batch = scaled_train_data[-n_timesteps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_eval_batch = first_eval_batch.reshape((1, n_timesteps, n_features))\n",
    "first_eval_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(first_eval_batch)\n",
    "# This results that, taking n_timesteps values, below given output must be the first value of the test data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step10: Predicting using RNN Model</ins></b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds the predections\n",
    "test_predictions = []\n",
    "\n",
    "# last n_timesteps points from the training dataset\n",
    "first_eval_batch = scaled_train_data[-n_timesteps:]\n",
    "# Reshaping it to the format RNN expects\n",
    "current_batch = first_eval_batch.reshape((1, n_timesteps, n_features))\n",
    "\n",
    "# How far to predict?\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    # store prediction\n",
    "    test_predictions.append(current_pred) \n",
    "    \n",
    "    # update batch to now include prediction and drop first value\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step11: Inverse Transformations and Comparison</b></ins></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predictions'] = true_predictions\n",
    "test.plot(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><ins><b>Step12: Saving and loading the model</b></ins></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./saved_models/LSTM_Alcohol_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"./saved_models/LSTM_Alcohol_Sales\")\n",
    "new_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

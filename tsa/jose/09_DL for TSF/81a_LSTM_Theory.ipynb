{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><b>LSMT</b></font>\n",
    "\n",
    "This [tutorial](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/) is divided into four parts; they are:\n",
    "\n",
    "- Univariate LSTM Models\n",
    "    - [Data Preparation](#Data_Preparation)\n",
    "    - [Vanilla LSTM](#Vanilla_LSTM)\n",
    "    - [Stacked_LSTM](#Stacked_LSTM)\n",
    "    - [Bidirectional_LSTM](#Bidirectional_LSTM)\n",
    "    - [CNN_LSTM](#CNN_LSTM)\n",
    "    - [ConvLSTM](#ConvLSTM)\n",
    "- [Multivariate LSTM Models](#Multivariate_LSTM_Models)\n",
    "    - Multiple Input Series\n",
    "    - Multiple Parallel Series\n",
    "- [Multi-Step LSTM Models](#Multi_Step_LSTM_Models)\n",
    "    - Data Preparation\n",
    "    - Vector Output Model\n",
    "    - Encoder-Decoder Model\n",
    "- Multivariate Multi-Step LSTM Models\n",
    "    - Multiple Input Multi-Step Output.\n",
    "    - Multiple Parallel Input and Multi-Step Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gold\"><b>Univariate LSTM Models</b></font>\n",
    "- [Data Preparation](#Data_Preparation)\n",
    "- [Vanilla LSTM](#Vanilla_LSTM)\n",
    "- [Stacked_LSTM](#Stacked_LSTM)\n",
    "- [Bidirectional_LSTM](#Bidirectional_LSTM)\n",
    "- [CNN_LSTM](#CNN_LSTM)\n",
    "- [ConvLSTM](#ConvLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_Preparation\"></a>\n",
    "## <font color=\"teal\"><b>Data Preparation</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[10 20 30 20 30 40 30 40 50 40 50 60 50 60 70 60 70 80] \n",
      "\n",
      "y:[40 50 60 70 80 90]\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "print(f\"X:\\n{X.flatten()} \\n\\ny:{y.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Vanilla_LSTM\"></a>\n",
    "## <font color=\"teal\"><b>Vanilla LSTM</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from tensorflow import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:[40 50 60 70 80 90] and yhat:[102.517494]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y:{y.flatten()} and yhat:{yhat.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Stacked_LSTM\"></a>\n",
    "## <font color=\"teal\"><b>Stacked LSTM</b></font>\n",
    "\n",
    "Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model\n",
    "> However, LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence\n",
    "\n",
    "`Stacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique\n",
    "It is the depth of neural networks that is generally attributed to the success of the approach on a wide range of challenging prediction problems`\n",
    "\n",
    "> Additional hidden layers can be added to a Multilayer Perceptron neural network to make it deeper<br>\n",
    "\n",
    "> The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction<br>\n",
    "\n",
    "> e.g. from lines to shapes to objects\n",
    "\n",
    "- _We can address this by setting the <font color=\"green\">return_sequences=True</font> argument on the layer and having the LSTM output a value for each time step in the input data_<br>\n",
    "- _This allows us to have 3D output from hidden LSTM layer as input to the next_\n",
    "\n",
    "We can therefore define a Stacked LSTM as follows:\n",
    "```python\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.34008]]\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# Pay attention to return_sequences=True\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "# 1 neuron per feature col and total features = n_features\n",
    "model.add(Dense(n_features))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "# Making 3D array for LSTM input\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"idirectional_LSTM\"></a>\n",
    "## <font color=\"teal\"><b>Bidirectional LSTM</b></font>\n",
    "On some sequence prediction problems, it can be beneficial to \n",
    "> allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations\n",
    "\n",
    "> We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional\n",
    "\n",
    "- __In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence.__<br><br>\n",
    "- __The first on the input sequence as-is and the second on a reversed copy of the input sequence__<br><br>\n",
    "- __This can provide additional context to the network and result in faster and even fuller learning on the problem.__\n",
    "\n",
    "1) Bidirectional LSTMs are supported in Keras via the Bidirectional layer wrapper <br>\n",
    " - This wrapper takes a recurrent layer (e.g. the first LSTM layer) as an argument <br>\n",
    " \n",
    "2) One could specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer. The options are:\n",
    "  - __‘sum‘__: The outputs are added together\n",
    "  - __‘mul‘__: The outputs are multiplied together\n",
    "  - __‘concat‘__: The outputs are concatenated together (the default), providing double the number of outputs to the next layer\n",
    "  - __‘ave‘__: The average of the outputs is taken<br>\n",
    "  \n",
    "  \n",
    "More Information in the [blog](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.624146]]\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CNN_LSTM\"></a>\n",
    "## <font color=\"teal\"><b>CNN LSTM</b></font>\n",
    "> A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data, spatial data\n",
    "\n",
    "`The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data`\n",
    "\n",
    "\n",
    "_CNN LSTMs(also called as Long-term RCN)_ are a class of models that is both spatially and temporally deep, and has the flexibility to be applied to a variety of vision tasks involving sequential inputs and outputs\n",
    "\n",
    "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images, like:\n",
    "- __Activity Recognition__: Generating a textual description of an activity demonstrated in a sequence of images\n",
    "- __Image Description__: Generating a textual description of a single image\n",
    "- __Video Description__: Generating a textual description of a sequence of images\n",
    "\n",
    "`\n",
    "CNN LSTM: A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret\n",
    "`\n",
    "> A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output <br>\n",
    "\n",
    "> It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps <br>\n",
    "\n",
    "___\n",
    "### <font color=\"yellow\">__CNN Model__</font>\n",
    "```python\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(1, (2,2), activation='relu', padding='same', input_shape=(10,10,1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "```\n",
    "\n",
    "- The snippet above expects to read in 10×10 pixel images with 1 channel (e.g. black and white)\n",
    "- The Conv2D will read the image in 2×2 snapshots and output one new 10×10 interpretation of the image\n",
    "- The MaxPooling2D will pool the interpretation into 2×2 blocks reducing the output to a 5×5 consolidation\n",
    "- The Flatten layer will take the single 5×5 map and transform it into a 25-element vector ready for some other layer to deal with, such as a Dense for outputting a prediction\n",
    "> All these makes sense for image classification and other computer vision tasks\n",
    "___\n",
    "\n",
    "### <font color=\"yellow\">__LSTM Model__</font>\n",
    "- The CNN model above is only capable of handling a single image, transforming it from input pixels into an internal matrix or vector representation\n",
    "- We need to repeat this operation across multiple images and allow the LSTM to build up internal state and update weights using BPTT across a sequence of the internal vector representations of input images\n",
    "- We want to apply the CNN model to each input image and pass on the output of each input image to the LSTM as a single time step\n",
    "- __We can achieve this by wrapping the entire CNN input model (one layer or more) in a TimeDistributed layer__\n",
    "\n",
    "```python\n",
    "model.add(TimeDistributed(...))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "```\n",
    "- __TimeDistributed layer__ layer achieves the desired outcome of applying the same layer or layers multiple times\n",
    "- In this case, applying it multiple times to multiple input time steps and in turn providing a sequence of “image interpretations” or “image features” to the LSTM model to work on\n",
    "\n",
    "___\n",
    "### <font color=\"yellow\">__CNN LSTM Model__</font>\n",
    "We can define a CNN LSTM model in Keras by first defining the CNN layer or layers, wrapping them in a TimeDistributed layer and then defining the LSTM and output layers\n",
    "```python\n",
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv2D(...))\n",
    "model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define LSTM model\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "```\n",
    "1) Split the input sequences into subsequences that can be processed by the CNN model\n",
    " - e.g., we can first split our univariate time series data into input/output samples with four steps as input and one as output <br>\n",
    " \n",
    "2) Each sample can then be split into two sub-samples, each with two time steps <br>\n",
    "3) The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input <br>\n",
    "\n",
    "```python\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "# number of subsequences as n_seq; number of time steps per subsequence as n_steps\n",
    "n_seq = 2; n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "```\n",
    "- We want to reuse the same CNN model when reading in each sub-sequence of data separately\n",
    "- This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence\n",
    "```python\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "```\n",
    "\n",
    "- The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified\n",
    "- The number of filters is the number of reads or interpretations of the input sequence\n",
    "- The kernel size is the number of time steps included of each ‘read’ operation of the input sequence\n",
    "- The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/2 of their size that includes the most salient features\n",
    "- These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer\n",
    "- Next, we can define the LSTM part of the model that interprets the CNN model’s read of the input sequence and makes a prediction\n",
    "```python\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.208145]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Flatten, TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Multivariate_LSTM_Models\"></a>\n",
    "# <font color=\"gold\"><b>Multivariate LSTM Models</b></font>\n",
    "- Multiple Input Series\n",
    "- Multiple Parallel Series\n",
    "<br>\n",
    "\n",
    "### __A problem may have:__\n",
    "- [x] two or more parallel input time series and\n",
    "- [x] an output time series that is dependent on the input time series\n",
    "> The input time series are parallel because each series has an observation at the same time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,  15,  25],\n",
       "       [ 20,  25,  45],\n",
       "       [ 30,  35,  65],\n",
       "       [ 40,  45,  85],\n",
       "       [ 50,  55, 105],\n",
       "       [ 60,  65, 125],\n",
       "       [ 70,  75, 145],\n",
       "       [ 80,  85, 165],\n",
       "       [ 90,  95, 185]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Multi_Step_LSTM_Models\"></a>\n",
    "# <font color=\"gold\"><b>Multi-Step LSTM Models</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

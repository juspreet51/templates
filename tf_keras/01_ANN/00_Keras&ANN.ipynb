{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step1)</font> Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step2)</font> GPU Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step3)</font> Data Processing For  ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Next, we create two empty lists. One will hold the input data, the other will hold the target data.\n",
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppose that an experimental drug was tested on individuals ranging from age 13 to 100 in a clinical trial. The trial had 2100 participants\n",
    "> 95% 0f 65+ had side effects\n",
    "> 95% of 65- didn't had side effects\n",
    "\n",
    "<font color=\"orange\">__So we want to build a model to tell us whether or not a patient will experience side effects solely based on the patient's age__</font>\n",
    "\n",
    "```\n",
    "This code creates 2100 samples and stores the age of the individuals in the train_samples list and stores whether or \n",
    "not the individuals experienced side effects in the train_labels list.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients who experienced side-effects\n",
    "for i in range(50):\n",
    "    # The ~5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)  # Age of patients: 13-64\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "\n",
    "    # The ~5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)  # Age of patients: 65-100\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "# Patients who didn't experienced side-effects\n",
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "\n",
    "    # The ~95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Train Samples: 2100\n",
      "22, 68, 13, 94, 31, 84, 14, 81, 17, 92, 53, 98, 33, 84, 32, \n",
      "Len of Train Labels: 2100\n",
      "1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, "
     ]
    }
   ],
   "source": [
    "print(\"Len of Train Samples:\",len(train_samples))\n",
    "for i in train_samples[:15]:\n",
    "    print(i, end=\", \")\n",
    "\n",
    "print(\"\\nLen of Train Labels:\",len(train_labels))\n",
    "for i in train_labels[:15]:\n",
    "    print(i, end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "We now convert both lists into numpy arrays due to what we discussed the fit() function expects\n",
    "<br> And we then shuffle the arrays to remove any order that was imposed on the data during the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)\n",
    "\n",
    "# We'll use scikit-learn’s MinMaxScaler class to scale all of the data down from a scale ranging from 13 to 100 to be on a scale from 0 to 1.\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\n",
    "# We reshape the data since the fit_transform() function, used in few next steps, doesn’t accept 1D data by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step4)</font> Create an Artificial Neural Network with TensorFlow's Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we could access the Dense module from Keras with the following import statement\n",
    "```\n",
    " from keras.layers import Dense\n",
    "```\n",
    "\n",
    "Now, using Keras with TensorFlow, the import statement looks like this:\n",
    "```\n",
    "from tensorflow.keras.layers import Dense\n",
    "```\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\">Before TensorFlow Integration</font>\n",
    "```\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "```\n",
    "\n",
    "## <font color=\"green\">After TensorFlow Integration</font>\n",
    "```\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">First, we need to import all the libraries we’ll be making use of</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note these modules would be used for creation of ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "\n",
    "# Note that we'll make use of these two modules in the next episode when we train the model.\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Build A Sequential Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model\" is an instance of a Sequential object\n",
    "# A tf.keras.Sequential model is a linear stack of layers\n",
    "# It accepts a list, and each element in the list should be a layer.\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Hidden Layer:\n",
    "- Our first layer is a <font color=\"red\">__Dense layer__</font>. This type of layer is our standard fully-connected or densely-connected neural network layer. <br>\n",
    "> Dense Layer will allow the connectivity between all the neurons from previous layers to be connected to next layer, i.e. yielding a dense interconnection <br>\n",
    "\n",
    "- The first required parameter that the Dense layer expects is the number of neurons or <font color=\"red\">__units__ </font> the layer has, and we’re arbitrarily setting this to 16.\n",
    "\n",
    "- Additionally, the model needs to know the ___shape of the input data___. For this reason, we specify the shape of the input data in the first hidden layer in the model (and ___only this layer___). The parameter called <font color=\"red\">__input_shape__</font> is how we specify this.\n",
    "\n",
    "- An optional parameter that we’ll set for the Dense layer is the <font color=\"red\">__activation function__</font> to use after this layer. We’ll use the popular choice of __relu__. Note, if you don’t explicitly set an activation function, then ___Keras will use the linear activation function as default activation function___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Hidden Layer\n",
    "Our next layer will also be a <font color=\"red\">__Dense layer__</font>, and this one will have <font color=\"red\">__32 nodes__</font>. The choice of how many neurons this node has is also arbitrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer\n",
    "Lastly, we specify the output layer. This layer is also a <font color=\"red\">__Dense layer__</font>, and it will have <font color=\"red\">__2 neurons__</font>. <br>\n",
    "This is because we have two possible outputs: _either a patient experienced side effects, or the patient did not experience side effects_.\n",
    "\n",
    "This time, the activation function we’ll use is <font color=\"red\">__softmax__</font>, which will give us a probability distribution among the possible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Note that we can call summary() on our model to get a quick visualization of it.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "__Next__: Next we’ll train this model on the data we created last time.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step5)</font> Train & Fit ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Training The Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling The Model\n",
    "# The first thing we need to do to get the model ready for training is call the compile() function on it.\n",
    "model1.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the optimizer <font color=\"teal\">__Adam__</font>. <font color=\"teal\">__Adam__</font> accepts an optional parameter <font color=\"teal\">__learning_rate__</font>, which we’ll set to 0.0001 <br>\n",
    "___Adam optimization is a stochastic gradient descent (SGD) method___\n",
    "\n",
    "The next parameter we specify is <font color=\"teal\">__loss__</font>. We’ll be using <font color=\"teal\">__sparse_categorical_crossentropy__</font>, given that _our labels are in integer format_\n",
    "\n",
    "when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use ___binary_crossentropy___ as our loss, rather than ___categorical_crossentropy___. Both options work equally well and achieve the exact same result.\n",
    "\n",
    "With binary_crossentropy, however, the last layer would need to use sigmoid, rather than softmax, as its activation function.\n",
    "\n",
    "The last parameter we specify in ___compile()___ is <font color=\"teal\">__metrics__</font> <br>\n",
    "This parameter expects a _list of metrics that we’d like to be evaluated by the model during training and testing_. We’ll set this to a list that contains the string ___‘accuracy’___.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Fitting the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Observations from result</font>\n",
    "The first item that we pass in to the fit() function is the training set __x__, i.e. _scaled_train_samples_ <br>\n",
    "The next parameter that we set is the labels for the training set __y__, which we previously gave the name _train_labels_\n",
    "\n",
    "We then specify the batch_size. Again, the concept of <font color=\"red\">__batch size__</font> is covered in detail in the [Deep Learning Fundamentals course](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU).\n",
    "\n",
    "Next, we specify how many <font color=\"red\">__epochs__</font> we want to run. We set this to 30. Note that an epoch is a single pass of all the data to the network.\n",
    "\n",
    "Lastly, we specify <font color=\"red\">__verbose=2__</font>. This just specifies how much output to the console we want to see during each epoch of training. The verbosity levels range from 0 to 2, so we’re getting the most verbose output.\n",
    "\n",
    "When we call <font color=\"red\">__fit()__</font>. on the model, the model trains, and we get this output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see corresponding output for each of the <font color=\"red\">__30 epochs__</font>. Judging by the <font color=\"red\">__loss and accuracy__</font>, we can see that both metrics steadily improve over time with accuracy reaching almost <font color=\"red\">__93%__</font> and loss steadily decreasing until we reach <font color=\"red\">__0.27__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step6)</font> Building A __Validation Set__\n",
    "After training a model, we want to make sure that our trained model could work fine also with new data that would be fed to it in future. <br>\n",
    "To assure this confirmity, we expose the data to a new set of values which our trianed model has never seen before and then we note how our data is performing, i,e, _checking if trained model is generalising well or not and what are it's accuracy with newly exposed data_ <br>\n",
    "\n",
    "This <font color=\"red\">__Validation set__</font> can be created by taking subset of the training data before performing any operation on it, and separate it as a different set and labeling it as ___Validation Set___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "189/189 - 0s - loss: 0.6806 - accuracy: 0.4783 - val_loss: 0.6649 - val_accuracy: 0.5143\n",
      "Epoch 2/30\n",
      "189/189 - 0s - loss: 0.6446 - accuracy: 0.5974 - val_loss: 0.6324 - val_accuracy: 0.6095\n",
      "Epoch 3/30\n",
      "189/189 - 0s - loss: 0.6117 - accuracy: 0.6825 - val_loss: 0.6004 - val_accuracy: 0.6857\n",
      "Epoch 4/30\n",
      "189/189 - 0s - loss: 0.5752 - accuracy: 0.7545 - val_loss: 0.5649 - val_accuracy: 0.7762\n",
      "Epoch 5/30\n",
      "189/189 - 0s - loss: 0.5412 - accuracy: 0.7894 - val_loss: 0.5331 - val_accuracy: 0.8095\n",
      "Epoch 6/30\n",
      "189/189 - 0s - loss: 0.5088 - accuracy: 0.8222 - val_loss: 0.5022 - val_accuracy: 0.8333\n",
      "Epoch 7/30\n",
      "189/189 - 0s - loss: 0.4775 - accuracy: 0.8429 - val_loss: 0.4725 - val_accuracy: 0.8476\n",
      "Epoch 8/30\n",
      "189/189 - 0s - loss: 0.4476 - accuracy: 0.8603 - val_loss: 0.4452 - val_accuracy: 0.8714\n",
      "Epoch 9/30\n",
      "189/189 - 0s - loss: 0.4200 - accuracy: 0.8661 - val_loss: 0.4202 - val_accuracy: 0.8857\n",
      "Epoch 10/30\n",
      "189/189 - 0s - loss: 0.3949 - accuracy: 0.8873 - val_loss: 0.3979 - val_accuracy: 0.8857\n",
      "Epoch 11/30\n",
      "189/189 - 1s - loss: 0.3732 - accuracy: 0.8921 - val_loss: 0.3795 - val_accuracy: 0.8952\n",
      "Epoch 12/30\n",
      "189/189 - 1s - loss: 0.3543 - accuracy: 0.8995 - val_loss: 0.3645 - val_accuracy: 0.9048\n",
      "Epoch 13/30\n",
      "189/189 - 0s - loss: 0.3384 - accuracy: 0.9106 - val_loss: 0.3512 - val_accuracy: 0.9048\n",
      "Epoch 14/30\n",
      "189/189 - 0s - loss: 0.3253 - accuracy: 0.9132 - val_loss: 0.3415 - val_accuracy: 0.9190\n",
      "Epoch 15/30\n",
      "189/189 - 0s - loss: 0.3147 - accuracy: 0.9233 - val_loss: 0.3332 - val_accuracy: 0.9190\n",
      "Epoch 16/30\n",
      "189/189 - 0s - loss: 0.3055 - accuracy: 0.9217 - val_loss: 0.3272 - val_accuracy: 0.9238\n",
      "Epoch 17/30\n",
      "189/189 - 0s - loss: 0.2981 - accuracy: 0.9280 - val_loss: 0.3217 - val_accuracy: 0.9238\n",
      "Epoch 18/30\n",
      "189/189 - 0s - loss: 0.2921 - accuracy: 0.9270 - val_loss: 0.3180 - val_accuracy: 0.9238\n",
      "Epoch 19/30\n",
      "189/189 - 0s - loss: 0.2869 - accuracy: 0.9317 - val_loss: 0.3137 - val_accuracy: 0.9238\n",
      "Epoch 20/30\n",
      "189/189 - 0s - loss: 0.2825 - accuracy: 0.9291 - val_loss: 0.3115 - val_accuracy: 0.9238\n",
      "Epoch 21/30\n",
      "189/189 - 0s - loss: 0.2786 - accuracy: 0.9286 - val_loss: 0.3109 - val_accuracy: 0.9286\n",
      "Epoch 22/30\n",
      "189/189 - 0s - loss: 0.2756 - accuracy: 0.9333 - val_loss: 0.3075 - val_accuracy: 0.9286\n",
      "Epoch 23/30\n",
      "189/189 - 1s - loss: 0.2729 - accuracy: 0.9360 - val_loss: 0.3048 - val_accuracy: 0.9238\n",
      "Epoch 24/30\n",
      "189/189 - 0s - loss: 0.2704 - accuracy: 0.9349 - val_loss: 0.3042 - val_accuracy: 0.9286\n",
      "Epoch 25/30\n",
      "189/189 - 0s - loss: 0.2684 - accuracy: 0.9376 - val_loss: 0.3020 - val_accuracy: 0.9286\n",
      "Epoch 26/30\n",
      "189/189 - 0s - loss: 0.2666 - accuracy: 0.9339 - val_loss: 0.3015 - val_accuracy: 0.9286\n",
      "Epoch 27/30\n",
      "189/189 - 0s - loss: 0.2647 - accuracy: 0.9402 - val_loss: 0.2990 - val_accuracy: 0.9238\n",
      "Epoch 28/30\n",
      "189/189 - 1s - loss: 0.2635 - accuracy: 0.9376 - val_loss: 0.2996 - val_accuracy: 0.9286\n",
      "Epoch 29/30\n",
      "189/189 - 1s - loss: 0.2622 - accuracy: 0.9376 - val_loss: 0.2992 - val_accuracy: 0.9286\n",
      "Epoch 30/30\n",
      "189/189 - 1s - loss: 0.2608 - accuracy: 0.9376 - val_loss: 0.2981 - val_accuracy: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223adf69fc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling The Model\n",
    "# The first thing we need to do to get the model ready for training is call the compile() function on it.\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=scaled_train_samples, y=train_labels, validation_split=0.1, batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> _Note that the fit() function shuffles the data before each epoch by default. When specifying the validation_split parameter, however, the validation data is selected from the last samples in the x and y data before shuffling._\n",
    "\n",
    "Therefore, in the case we're using validation_split in this way to create our validation data, ___we need to be sure that our data has been shuffled ahead of time, like we previously did in <font color=\"yellow\">Step3</font>___\n",
    "\n",
    "> We can now see not only how well our model is learning the features of the training data, but also how well the model is generalizing to new, unseen data from the validation set\n",
    "> Next, we'll see how to use our model for inference\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step7)</font> __ANN Predictions__\n",
    "When we train a model, the hope is that we’ll later be able to take the trained model, apply it to new data, and have the model generalize and accurately predict on data it hasn’t seen before.\n",
    "\n",
    "- For example, suppose we have a model that categorizes images of cats or dogs and that the training data contained thousands of images of cats and dogs from a particular data set online.\n",
    "\n",
    "- Now suppose, that later we want to take this model and use it to predict on other images of cats and dogs from a different data set. The hope is that, even though our model wasn’t exposed to these particular dog and cat images during training, it will still be able to accurately make predictions for them based on what it’s learned from the cat and dog data set from which it was trained.\n",
    "\n",
    "We call this process <font color=\"red\">_inference_</font>, as the model is using its knowledge gained from training and using it to infer a prediction or result.\n",
    "\n",
    "Given the results we’ve seen from the validation data, it appears that this model should do well on predicting on a new test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Creating The Test Set</font>\n",
    "Similar to <font color=\"yellow\">Step3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels =  []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "\n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "\n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Evaluating The Test Set</font>\n",
    "To get predictions from the model for the test set, we call <font color=\"red\">__model.predict()__</font>\n",
    "> Note that, unlike with training and validation sets, we do not pass the labels of the test set to the model during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(\n",
    "      x=scaled_test_samples\n",
    "    , batch_size=10\n",
    "    , verbose=1\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the predictions list is itself a list of length 2. \n",
    "> The sum of the two values in each list is 1\n",
    "\n",
    "The reason for this is because <font color=\"green\">__the two columns contain probabilities for each possible output: experienced side effects and did not experience side effects__</font>\n",
    "\n",
    "- The first column contains the probability for each patient not experiencing side effects <br>\n",
    "> <font color=\"red\">not experiencing side effects</font> is represented by a 0 <br>\n",
    "- The second column contains the probability for each patient experiencing side effects <br>\n",
    "> <font color=\"red\">experiencing side effects</font> is represented by a 1 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05263274 0.9473672 ]\n",
      "[0.1816221 0.818378 ]\n",
      "[0.96497136 0.0350286 ]\n",
      "[0.05979005 0.94021   ]\n",
      "[0.03812337 0.96187663]\n",
      "[0.05979005 0.94021   ]\n",
      "[0.16120909 0.83879095]\n",
      "[0.9424778 0.0575221]\n",
      "[0.9694109  0.03058911]\n",
      "[0.9699055  0.03009455]\n",
      "[0.07720424 0.9227957 ]\n",
      "[0.9684544  0.03154553]\n",
      "[0.97001517 0.02998481]\n",
      "[0.9620686  0.03793141]\n",
      "[0.02410694 0.9758931 ]\n"
     ]
    }
   ],
   "source": [
    "for i in predictions[:15]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of most probable prediction for each sample: [1 1 0 1 1 1 1 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Most probable prediction for each sample\n",
    "rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "print(\"Index of most probable prediction for each sample:\",rounded_predictions[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can observe the underlying predictions from the model, however, we cannot judge how accurate these predictions are just by looking at the predicted output.\n",
    "\n",
    "If we have corresponding labels for the test set,and compare these labels to the predicted labels, <br>\n",
    "> We can visualize the accuracy of the model's evaluations via a visualizing tool called <font color=\"red\">___confusion matrix___</font>\n",
    "\n",
    "This will aid us in being able to visually observe how well a neural network is predicting during inference.\n",
    "\n",
    "<img src=\"./imgs/confusion_matri_interpretation.jpeg\" style=\"width:500px\"> <br>\n",
    "\n",
    "- <font color=\"red\">__FN__: Prediction is Negative, but that is a False Prediction </font>\n",
    "- <font color=\"red\">__FP__: Prediction is Positive, but that is a False Prediction </font>\n",
    "- <font color=\"green\">__TP__: Prediction is Positive, and that is a True Prediction </font>\n",
    "- <font color=\"green\">__TN__: Prediction is Negative, and that is a True Prediction </font>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\"> Step8)</font> Plotting A Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we import all the required libraries we’ll be working with.\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">The confusion matrix we’ll be plotting comes from scikit-learn </font>\n",
    "\n",
    "We then create the confusion matrix and assign it to the variable <font color=\"red\">__cm__<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the confusion matrix, we pass in the true labels test_labels as well as the network’s predicted labels rounded_predictions for the test set\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">We have a function called plot_confusion_matrix() that came directly from scikit-learn</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Next, we define the labels for the confusion matrix. In our case, the labels are titled “no side effects” and “had side effects.”</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['no_side_effects','had_side_effects']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Lastly, we plot the confusion matrix by using the plot_confusion_matrix() function we just defined </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[187  23]\n",
      " [  9 201]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">Reading A Confusion Matrix</font>\n",
    "- We have the predicted labels on the x-axis and the true labels on the y-axis <br>\n",
    "- The blue cells running from the top left to bottom right contain the number of samples that the model accurately predicted <br>\n",
    "- The white cells contain the number of samples that were incorrectly predicted <br>\n",
    "\n",
    "#### We can see that the <font color=\"green\">model accurately predicted 397 and incorrectly predicted 23</font> out of the 420 total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">Step9)</font> Save And Load A Model\n",
    "If we want to save a model at its current state after it was trained so that we could make use of it later, we can call the <font color=\"red\">save()</font> function on the model. <br>\n",
    "To save(), we pass in the file path and name of the file we want to save the model to with an <font color=\"red\">.h5 extension</font>\n",
    "- Note, this function also allows for saving the model as a _Tensorflow <font color=\"green\">SavedModel</font>_ as well if you'd prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">9.a) Model.Save</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This __save__ function saves:\n",
    "- Architecture of the model\n",
    "- Weights of the model\n",
    "- Training Configuration (loss, optimizer)\n",
    "- Supports resuming the training of the model from the point it was saved, by saving the State of the optimizer\n",
    "\n",
    "## <font color=\"orange\">Now that we have this model saved, we can load the model at a later time</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('saved_models/medical_trial_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available alternatives:\n",
    "- json Architecture, i.e. This will not save the model weights, configurations, optimizer, loss or anything else. This only saves the architecture of the model\n",
    "\n",
    "```sh\n",
    "json_string = model.to_json()\n",
    "json_string\n",
    "```\n",
    "\n",
    "```sh\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "model_architecture.summary()\n",
    "```\n",
    "- Similarly yaml_string\n",
    "```\n",
    "yaml_string = model.to_yaml() and model_from_yaml()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">9.b) Model.to_json</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 1]}, \"keras_version\": \"2.3.0-tf\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = model.to_json()\n",
    "json_string\n",
    "# Displays the details of the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "model_architecture.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">9.c) Saving And Loading The Weights Of The Model</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('saved_models/medical_trial_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At a later point, we could then load the saved weights in to a new model, but the new model will need to have the same architecture as the old model before the weights can be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.load_weights('saved_models/medical_trial_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
